{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC #LinearSVC should scale better to a higher number of samples than SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Conv1D, MaxPooling1D, Bidirectional\n",
    "from keras.utils import np_utils\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from transformers import RobertaTokenizer, TFRobertaModel\n",
    "from transformers import XLNetTokenizer, TFXLNetModel\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "pd.set_option('display.max_colwidth', 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.pt import Portuguese\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"pt_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spaCy language model for European Portuguese\n",
    "#nlp = Portuguese()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(df):\n",
    "    l=[]\n",
    "    columns=df.columns\n",
    "    for col in columns:\n",
    "        dtypes=df[col].dtypes\n",
    "        nunique=df[col].nunique()\n",
    "        sum_null=df[col].isnull().sum()\n",
    "        l.append([col,dtypes,nunique,sum_null])\n",
    "    df_check=pd.DataFrame(l)\n",
    "    df_check.columns=['column','dtypes','nunique','sum_null']\n",
    "    return df_check "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Load CSV file into a Pandas DataFrame and clean it (missing, null and duplicated data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Source</th>\n",
       "      <th>URL</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O SNS está um caos completo Onde estão os gran...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.direitapolitica.com/o-sns-esta-um-...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Está a circular no Facebook uma imagem que sup...</td>\n",
       "      <td>facebook</td>\n",
       "      <td>https://poligrafo.sapo.pt/fact-check/fact-chec...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Covid-19: Espanha decreta \"fim da crise sanitá...</td>\n",
       "      <td>SIC Noticias</td>\n",
       "      <td>https://sicnoticias.pt/especiais/coronavirus/2...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tem medo de dentistas? Conheça o SNAP-ON SMILE...</td>\n",
       "      <td>amilcar freitas</td>\n",
       "      <td>https://arquivo.pt/wayback/20160603024458mp_/h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cientistas descobrem a razão que leva os bebés...</td>\n",
       "      <td>SIC Noticias</td>\n",
       "      <td>https://sicnoticias.pt/mundo/2018-03-12-Cienti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text           Source  \\\n",
       "0  O SNS está um caos completo Onde estão os gran...              NaN   \n",
       "1  Está a circular no Facebook uma imagem que sup...         facebook   \n",
       "2  Covid-19: Espanha decreta \"fim da crise sanitá...     SIC Noticias   \n",
       "3  Tem medo de dentistas? Conheça o SNAP-ON SMILE...  amilcar freitas   \n",
       "4  Cientistas descobrem a razão que leva os bebés...     SIC Noticias   \n",
       "\n",
       "                                                 URL  Label  \n",
       "0  https://www.direitapolitica.com/o-sns-esta-um-...      0  \n",
       "1  https://poligrafo.sapo.pt/fact-check/fact-chec...      1  \n",
       "2  https://sicnoticias.pt/especiais/coronavirus/2...      1  \n",
       "3  https://arquivo.pt/wayback/20160603024458mp_/h...      0  \n",
       "4  https://sicnoticias.pt/mundo/2018-03-12-Cienti...      1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data from CSV file\n",
    "import pandas as pd\n",
    "df = pd.read_csv('Final_dataset_portuguese.csv').sample(10000).reset_index(drop='index')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column</th>\n",
       "      <th>dtypes</th>\n",
       "      <th>nunique</th>\n",
       "      <th>sum_null</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Text</td>\n",
       "      <td>object</td>\n",
       "      <td>8938</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Source</td>\n",
       "      <td>object</td>\n",
       "      <td>842</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>URL</td>\n",
       "      <td>object</td>\n",
       "      <td>10000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Label</td>\n",
       "      <td>int64</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   column  dtypes  nunique  sum_null\n",
       "0    Text  object     8938         0\n",
       "1  Source  object      842      2006\n",
       "2     URL  object    10000         0\n",
       "3   Label   int64        2         0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.drop(columns=['Unnamed: 0'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete missing data\n",
    "#df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Preprocess the text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('portuguese'))\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "df['text_preprocessed'] = df['Text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop_words = set(stopwords.words('portuguese'))\n",
    "#df['title'] = df['title'].apply(lambda x: ' '.join([word for word in word_tokenize(x) if word.lower() not in stop_words]))\n",
    "#df['text_preprocessed'] = df['text'].apply(lambda x: ' '.join([word for word in word_tokenize(x) if word.lower() not in stop_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Source</th>\n",
       "      <th>URL</th>\n",
       "      <th>Label</th>\n",
       "      <th>text_preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O SNS está um caos completo Onde estão os gran...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.direitapolitica.com/o-sns-esta-um-...</td>\n",
       "      <td>0</td>\n",
       "      <td>SNS caos completo Onde grandoleiros ? outro go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Está a circular no Facebook uma imagem que sup...</td>\n",
       "      <td>facebook</td>\n",
       "      <td>https://poligrafo.sapo.pt/fact-check/fact-chec...</td>\n",
       "      <td>1</td>\n",
       "      <td>circular Facebook imagem supostamente demonstr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Covid-19: Espanha decreta \"fim da crise sanitá...</td>\n",
       "      <td>SIC Noticias</td>\n",
       "      <td>https://sicnoticias.pt/especiais/coronavirus/2...</td>\n",
       "      <td>1</td>\n",
       "      <td>Covid-19 : Espanha decreta `` fim crise sanitá...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tem medo de dentistas? Conheça o SNAP-ON SMILE...</td>\n",
       "      <td>amilcar freitas</td>\n",
       "      <td>https://arquivo.pt/wayback/20160603024458mp_/h...</td>\n",
       "      <td>0</td>\n",
       "      <td>medo dentistas ? Conheça SNAP-ON SMILE Snap-On...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cientistas descobrem a razão que leva os bebés...</td>\n",
       "      <td>SIC Noticias</td>\n",
       "      <td>https://sicnoticias.pt/mundo/2018-03-12-Cienti...</td>\n",
       "      <td>1</td>\n",
       "      <td>Cientistas descobrem razão leva bebés mexerem-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text           Source  \\\n",
       "0  O SNS está um caos completo Onde estão os gran...              NaN   \n",
       "1  Está a circular no Facebook uma imagem que sup...         facebook   \n",
       "2  Covid-19: Espanha decreta \"fim da crise sanitá...     SIC Noticias   \n",
       "3  Tem medo de dentistas? Conheça o SNAP-ON SMILE...  amilcar freitas   \n",
       "4  Cientistas descobrem a razão que leva os bebés...     SIC Noticias   \n",
       "\n",
       "                                                 URL  Label  \\\n",
       "0  https://www.direitapolitica.com/o-sns-esta-um-...      0   \n",
       "1  https://poligrafo.sapo.pt/fact-check/fact-chec...      1   \n",
       "2  https://sicnoticias.pt/especiais/coronavirus/2...      1   \n",
       "3  https://arquivo.pt/wayback/20160603024458mp_/h...      0   \n",
       "4  https://sicnoticias.pt/mundo/2018-03-12-Cienti...      1   \n",
       "\n",
       "                                   text_preprocessed  \n",
       "0  SNS caos completo Onde grandoleiros ? outro go...  \n",
       "1  circular Facebook imagem supostamente demonstr...  \n",
       "2  Covid-19 : Espanha decreta `` fim crise sanitá...  \n",
       "3  medo dentistas ? Conheça SNAP-ON SMILE Snap-On...  \n",
       "4  Cientistas descobrem razão leva bebés mexerem-...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemmer = PorterStemmer()\n",
    "\n",
    "# #df['title'] = df['title'].apply(lambda x: ' '.join([stemmer.stem(word) for word in word_tokenize(x)]))\n",
    "# df['text_preprocessed'] = df['text_preprocessed'].apply(lambda x: ' '.join([stemmer.stem(word) for word in word_tokenize(x)]))\n",
    "\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for lemmatization\n",
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    lemmatized_tokens = [token.lemma_ for token in doc]\n",
    "    return \" \".join(lemmatized_tokens)\n",
    "\n",
    "df['text_preprocessed'] = df['text_preprocessed'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Source</th>\n",
       "      <th>URL</th>\n",
       "      <th>Label</th>\n",
       "      <th>text_preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O SNS está um caos completo Onde estão os gran...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.direitapolitica.com/o-sns-esta-um-...</td>\n",
       "      <td>0</td>\n",
       "      <td>SNS caos completo onde Grandoleiros ? outro go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Está a circular no Facebook uma imagem que sup...</td>\n",
       "      <td>facebook</td>\n",
       "      <td>https://poligrafo.sapo.pt/fact-check/fact-chec...</td>\n",
       "      <td>1</td>\n",
       "      <td>circular Facebook imagem supostamente demonstr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Covid-19: Espanha decreta \"fim da crise sanitá...</td>\n",
       "      <td>SIC Noticias</td>\n",
       "      <td>https://sicnoticias.pt/especiais/coronavirus/2...</td>\n",
       "      <td>1</td>\n",
       "      <td>Covid-19 : Espanha decretar ` ` fim crise sani...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tem medo de dentistas? Conheça o SNAP-ON SMILE...</td>\n",
       "      <td>amilcar freitas</td>\n",
       "      <td>https://arquivo.pt/wayback/20160603024458mp_/h...</td>\n",
       "      <td>0</td>\n",
       "      <td>medo dentista ? Conheça sNAP-ON SMILE Snap-On ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cientistas descobrem a razão que leva os bebés...</td>\n",
       "      <td>SIC Noticias</td>\n",
       "      <td>https://sicnoticias.pt/mundo/2018-03-12-Cienti...</td>\n",
       "      <td>1</td>\n",
       "      <td>cientista descobrer razão levar bebé mexerer s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text           Source  \\\n",
       "0  O SNS está um caos completo Onde estão os gran...              NaN   \n",
       "1  Está a circular no Facebook uma imagem que sup...         facebook   \n",
       "2  Covid-19: Espanha decreta \"fim da crise sanitá...     SIC Noticias   \n",
       "3  Tem medo de dentistas? Conheça o SNAP-ON SMILE...  amilcar freitas   \n",
       "4  Cientistas descobrem a razão que leva os bebés...     SIC Noticias   \n",
       "\n",
       "                                                 URL  Label  \\\n",
       "0  https://www.direitapolitica.com/o-sns-esta-um-...      0   \n",
       "1  https://poligrafo.sapo.pt/fact-check/fact-chec...      1   \n",
       "2  https://sicnoticias.pt/especiais/coronavirus/2...      1   \n",
       "3  https://arquivo.pt/wayback/20160603024458mp_/h...      0   \n",
       "4  https://sicnoticias.pt/mundo/2018-03-12-Cienti...      1   \n",
       "\n",
       "                                   text_preprocessed  \n",
       "0  SNS caos completo onde Grandoleiros ? outro go...  \n",
       "1  circular Facebook imagem supostamente demonstr...  \n",
       "2  Covid-19 : Espanha decretar ` ` fim crise sani...  \n",
       "3  medo dentista ? Conheça sNAP-ON SMILE Snap-On ...  \n",
       "4  cientista descobrer razão levar bebé mexerer s...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk import ngrams\n",
    "\n",
    "#df['title_ngrams'] = df['title'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in word_tokenize(x)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional linguistic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    words = word_tokenize(text)\n",
    "    return len(words)\n",
    "\n",
    "def count_nouns(text):\n",
    "    doc = nlp(text)\n",
    "    noun_count = sum([1 for token in doc if token.pos_ == 'NOUN'])\n",
    "    return noun_count\n",
    "\n",
    "def count_verbs(text):\n",
    "    doc = nlp(text)\n",
    "    verb_count = sum([1 for token in doc if token.pos_ == 'VERB'])\n",
    "    return verb_count\n",
    "\n",
    "def count_adjectives(text):\n",
    "    doc = nlp(text)\n",
    "    adj_count = sum([1 for token in doc if token.pos_ == 'ADJ'])\n",
    "    return adj_count\n",
    "\n",
    "def count_adverbs(text):\n",
    "    doc = nlp(text)\n",
    "    adv_count = sum([1 for token in doc if token.pos_ == 'ADV'])\n",
    "    return adv_count\n",
    "\n",
    "df['count_words_text'] = df['text_preprocessed'].apply(count_words)\n",
    "df['num_nouns_text'] = df['text_preprocessed'].apply(count_nouns)\n",
    "df['num_verbs_text'] = df['text_preprocessed'].apply(count_verbs)\n",
    "df['num_adj_text'] = df['text_preprocessed'].apply(count_adjectives)\n",
    "df['num_adv_text'] = df['text_preprocessed'].apply(count_adverbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Sentiment score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_avg_sentiment_score(text):\n",
    "    return sia.polarity_scores(text)['compound']\n",
    "\n",
    "def get_pos_sentiment_score(text):\n",
    "    return sia.polarity_scores(text)['pos']\n",
    "\n",
    "def get_neu_sentiment_score(text):\n",
    "    return sia.polarity_scores(text)['neu']\n",
    "\n",
    "def get_neg_sentiment_score(text):\n",
    "    return sia.polarity_scores(text)['neg']\n",
    "\n",
    "df['sentiment_avg_text'] = df['text_preprocessed'].apply(lambda x: get_avg_sentiment_score(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Create final feature column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the preprocessed text, title, sentiment, and additional features\n",
    "df['features'] = df.apply(lambda x: ' '.join([x['text_preprocessed'],\n",
    "                                              str(x['sentiment_avg_text']),\n",
    "                                              str(x['count_words_text']), \n",
    "                                              str(x['num_nouns_text']),\n",
    "                                              str(x['num_verbs_text']),\n",
    "                                              str(x['num_adj_text']),\n",
    "                                              str(x['num_adv_text']),\n",
    "                                              str(x[\"Source\"])]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Source</th>\n",
       "      <th>URL</th>\n",
       "      <th>Label</th>\n",
       "      <th>text_preprocessed</th>\n",
       "      <th>count_words_text</th>\n",
       "      <th>num_nouns_text</th>\n",
       "      <th>num_verbs_text</th>\n",
       "      <th>num_adj_text</th>\n",
       "      <th>num_adv_text</th>\n",
       "      <th>sentiment_avg_text</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O SNS está um caos completo Onde estão os gran...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.direitapolitica.com/o-sns-esta-um-...</td>\n",
       "      <td>0</td>\n",
       "      <td>SNS caos completo onde Grandoleiros ? outro go...</td>\n",
       "      <td>71</td>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>SNS caos completo onde Grandoleiros ? outro go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Está a circular no Facebook uma imagem que sup...</td>\n",
       "      <td>facebook</td>\n",
       "      <td>https://poligrafo.sapo.pt/fact-check/fact-chec...</td>\n",
       "      <td>1</td>\n",
       "      <td>circular Facebook imagem supostamente demonstr...</td>\n",
       "      <td>108</td>\n",
       "      <td>33</td>\n",
       "      <td>25</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.2500</td>\n",
       "      <td>circular Facebook imagem supostamente demonstr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Covid-19: Espanha decreta \"fim da crise sanitá...</td>\n",
       "      <td>SIC Noticias</td>\n",
       "      <td>https://sicnoticias.pt/especiais/coronavirus/2...</td>\n",
       "      <td>1</td>\n",
       "      <td>Covid-19 : Espanha decretar ` ` fim crise sani...</td>\n",
       "      <td>405</td>\n",
       "      <td>113</td>\n",
       "      <td>60</td>\n",
       "      <td>48</td>\n",
       "      <td>11</td>\n",
       "      <td>0.3818</td>\n",
       "      <td>Covid-19 : Espanha decretar ` ` fim crise sani...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tem medo de dentistas? Conheça o SNAP-ON SMILE...</td>\n",
       "      <td>amilcar freitas</td>\n",
       "      <td>https://arquivo.pt/wayback/20160603024458mp_/h...</td>\n",
       "      <td>0</td>\n",
       "      <td>medo dentista ? Conheça sNAP-ON SMILE Snap-On ...</td>\n",
       "      <td>352</td>\n",
       "      <td>103</td>\n",
       "      <td>48</td>\n",
       "      <td>68</td>\n",
       "      <td>14</td>\n",
       "      <td>0.9662</td>\n",
       "      <td>medo dentista ? Conheça sNAP-ON SMILE Snap-On ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cientistas descobrem a razão que leva os bebés...</td>\n",
       "      <td>SIC Noticias</td>\n",
       "      <td>https://sicnoticias.pt/mundo/2018-03-12-Cienti...</td>\n",
       "      <td>1</td>\n",
       "      <td>cientista descobrer razão levar bebé mexerer s...</td>\n",
       "      <td>213</td>\n",
       "      <td>71</td>\n",
       "      <td>50</td>\n",
       "      <td>37</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2960</td>\n",
       "      <td>cientista descobrer razão levar bebé mexerer s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text           Source  \\\n",
       "0  O SNS está um caos completo Onde estão os gran...              NaN   \n",
       "1  Está a circular no Facebook uma imagem que sup...         facebook   \n",
       "2  Covid-19: Espanha decreta \"fim da crise sanitá...     SIC Noticias   \n",
       "3  Tem medo de dentistas? Conheça o SNAP-ON SMILE...  amilcar freitas   \n",
       "4  Cientistas descobrem a razão que leva os bebés...     SIC Noticias   \n",
       "\n",
       "                                                 URL  Label  \\\n",
       "0  https://www.direitapolitica.com/o-sns-esta-um-...      0   \n",
       "1  https://poligrafo.sapo.pt/fact-check/fact-chec...      1   \n",
       "2  https://sicnoticias.pt/especiais/coronavirus/2...      1   \n",
       "3  https://arquivo.pt/wayback/20160603024458mp_/h...      0   \n",
       "4  https://sicnoticias.pt/mundo/2018-03-12-Cienti...      1   \n",
       "\n",
       "                                   text_preprocessed  count_words_text  \\\n",
       "0  SNS caos completo onde Grandoleiros ? outro go...                71   \n",
       "1  circular Facebook imagem supostamente demonstr...               108   \n",
       "2  Covid-19 : Espanha decretar ` ` fim crise sani...               405   \n",
       "3  medo dentista ? Conheça sNAP-ON SMILE Snap-On ...               352   \n",
       "4  cientista descobrer razão levar bebé mexerer s...               213   \n",
       "\n",
       "   num_nouns_text  num_verbs_text  num_adj_text  num_adv_text  \\\n",
       "0              16              14             7             6   \n",
       "1              33              25            11             3   \n",
       "2             113              60            48            11   \n",
       "3             103              48            68            14   \n",
       "4              71              50            37            10   \n",
       "\n",
       "   sentiment_avg_text                                           features  \n",
       "0              0.0000  SNS caos completo onde Grandoleiros ? outro go...  \n",
       "1             -0.2500  circular Facebook imagem supostamente demonstr...  \n",
       "2              0.3818  Covid-19 : Espanha decretar ` ` fim crise sani...  \n",
       "3              0.9662  medo dentista ? Conheça sNAP-ON SMILE Snap-On ...  \n",
       "4              0.2960  cientista descobrer razão levar bebé mexerer s...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Final_preprocessed_dataset_portuguese.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpy-backup15",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
